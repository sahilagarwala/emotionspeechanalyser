{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3bc0b46-358a-4c87-b630-d2cde2a14b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3d99e73-a14b-4fbc-9dae-8646d0c416c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>mfcc_1</th>\n",
       "      <th>mfcc_2</th>\n",
       "      <th>mfcc_3</th>\n",
       "      <th>mfcc_4</th>\n",
       "      <th>mfcc_5</th>\n",
       "      <th>mfcc_6</th>\n",
       "      <th>mfcc_7</th>\n",
       "      <th>mfcc_8</th>\n",
       "      <th>mfcc_9</th>\n",
       "      <th>mfcc_10</th>\n",
       "      <th>mfcc_11</th>\n",
       "      <th>mfcc_12</th>\n",
       "      <th>mfcc_13</th>\n",
       "      <th>class_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-596.27124</td>\n",
       "      <td>105.507520</td>\n",
       "      <td>-16.348253</td>\n",
       "      <td>34.213352</td>\n",
       "      <td>-7.040627</td>\n",
       "      <td>9.673249</td>\n",
       "      <td>15.672894</td>\n",
       "      <td>-18.594084</td>\n",
       "      <td>12.680883</td>\n",
       "      <td>-3.053727</td>\n",
       "      <td>-9.320203</td>\n",
       "      <td>5.608082</td>\n",
       "      <td>-5.410851</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-359.78244</td>\n",
       "      <td>72.187680</td>\n",
       "      <td>27.768223</td>\n",
       "      <td>38.328430</td>\n",
       "      <td>5.198799</td>\n",
       "      <td>8.380541</td>\n",
       "      <td>10.607283</td>\n",
       "      <td>-13.184477</td>\n",
       "      <td>11.505762</td>\n",
       "      <td>-4.186356</td>\n",
       "      <td>-6.031159</td>\n",
       "      <td>2.438568</td>\n",
       "      <td>-6.354579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-424.57343</td>\n",
       "      <td>57.615242</td>\n",
       "      <td>12.128860</td>\n",
       "      <td>49.243443</td>\n",
       "      <td>-2.526363</td>\n",
       "      <td>11.702657</td>\n",
       "      <td>11.698957</td>\n",
       "      <td>-2.770733</td>\n",
       "      <td>-1.388484</td>\n",
       "      <td>-1.123289</td>\n",
       "      <td>-11.638446</td>\n",
       "      <td>14.478795</td>\n",
       "      <td>-10.840614</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-524.17444</td>\n",
       "      <td>103.392075</td>\n",
       "      <td>-7.327183</td>\n",
       "      <td>56.391010</td>\n",
       "      <td>10.343275</td>\n",
       "      <td>4.731972</td>\n",
       "      <td>23.960910</td>\n",
       "      <td>-0.654973</td>\n",
       "      <td>6.595408</td>\n",
       "      <td>4.164212</td>\n",
       "      <td>-6.080491</td>\n",
       "      <td>7.985336</td>\n",
       "      <td>0.112556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-519.28940</td>\n",
       "      <td>101.250860</td>\n",
       "      <td>20.647596</td>\n",
       "      <td>29.956661</td>\n",
       "      <td>8.789474</td>\n",
       "      <td>8.442513</td>\n",
       "      <td>3.100049</td>\n",
       "      <td>5.523725</td>\n",
       "      <td>0.429982</td>\n",
       "      <td>5.303805</td>\n",
       "      <td>-0.014299</td>\n",
       "      <td>4.767339</td>\n",
       "      <td>0.963716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>-473.95490</td>\n",
       "      <td>94.092310</td>\n",
       "      <td>20.465342</td>\n",
       "      <td>14.455287</td>\n",
       "      <td>0.151350</td>\n",
       "      <td>3.607172</td>\n",
       "      <td>-2.468565</td>\n",
       "      <td>-5.190783</td>\n",
       "      <td>0.989746</td>\n",
       "      <td>-2.069601</td>\n",
       "      <td>-10.654523</td>\n",
       "      <td>0.043760</td>\n",
       "      <td>2.364469</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>-569.43933</td>\n",
       "      <td>38.809494</td>\n",
       "      <td>15.469359</td>\n",
       "      <td>35.134655</td>\n",
       "      <td>-19.263390</td>\n",
       "      <td>-2.875730</td>\n",
       "      <td>-5.993804</td>\n",
       "      <td>-12.542236</td>\n",
       "      <td>-8.285859</td>\n",
       "      <td>-0.797593</td>\n",
       "      <td>-6.813976</td>\n",
       "      <td>-10.226942</td>\n",
       "      <td>-7.345919</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>-534.11633</td>\n",
       "      <td>45.754597</td>\n",
       "      <td>6.438214</td>\n",
       "      <td>24.977623</td>\n",
       "      <td>1.711067</td>\n",
       "      <td>9.408805</td>\n",
       "      <td>0.141316</td>\n",
       "      <td>-0.821431</td>\n",
       "      <td>-1.418567</td>\n",
       "      <td>3.832415</td>\n",
       "      <td>-2.813458</td>\n",
       "      <td>-1.262796</td>\n",
       "      <td>-4.410855</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>-552.32140</td>\n",
       "      <td>65.580140</td>\n",
       "      <td>21.206110</td>\n",
       "      <td>12.841641</td>\n",
       "      <td>-0.550191</td>\n",
       "      <td>12.242992</td>\n",
       "      <td>-0.532321</td>\n",
       "      <td>-1.793944</td>\n",
       "      <td>2.008286</td>\n",
       "      <td>-4.199604</td>\n",
       "      <td>-3.661440</td>\n",
       "      <td>-2.616800</td>\n",
       "      <td>-4.645094</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>-544.26825</td>\n",
       "      <td>55.101284</td>\n",
       "      <td>16.550330</td>\n",
       "      <td>30.429848</td>\n",
       "      <td>-6.519744</td>\n",
       "      <td>12.863327</td>\n",
       "      <td>-10.227871</td>\n",
       "      <td>8.540395</td>\n",
       "      <td>-4.947164</td>\n",
       "      <td>0.367120</td>\n",
       "      <td>-0.755701</td>\n",
       "      <td>1.520880</td>\n",
       "      <td>-5.485261</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0     mfcc_1      mfcc_2     mfcc_3     mfcc_4     mfcc_5  \\\n",
       "0            0 -596.27124  105.507520 -16.348253  34.213352  -7.040627   \n",
       "1            1 -359.78244   72.187680  27.768223  38.328430   5.198799   \n",
       "2            2 -424.57343   57.615242  12.128860  49.243443  -2.526363   \n",
       "3            3 -524.17444  103.392075  -7.327183  56.391010  10.343275   \n",
       "4            4 -519.28940  101.250860  20.647596  29.956661   8.789474   \n",
       "..         ...        ...         ...        ...        ...        ...   \n",
       "63          63 -473.95490   94.092310  20.465342  14.455287   0.151350   \n",
       "64          64 -569.43933   38.809494  15.469359  35.134655 -19.263390   \n",
       "65          65 -534.11633   45.754597   6.438214  24.977623   1.711067   \n",
       "66          66 -552.32140   65.580140  21.206110  12.841641  -0.550191   \n",
       "67          67 -544.26825   55.101284  16.550330  30.429848  -6.519744   \n",
       "\n",
       "       mfcc_6     mfcc_7     mfcc_8     mfcc_9   mfcc_10    mfcc_11  \\\n",
       "0    9.673249  15.672894 -18.594084  12.680883 -3.053727  -9.320203   \n",
       "1    8.380541  10.607283 -13.184477  11.505762 -4.186356  -6.031159   \n",
       "2   11.702657  11.698957  -2.770733  -1.388484 -1.123289 -11.638446   \n",
       "3    4.731972  23.960910  -0.654973   6.595408  4.164212  -6.080491   \n",
       "4    8.442513   3.100049   5.523725   0.429982  5.303805  -0.014299   \n",
       "..        ...        ...        ...        ...       ...        ...   \n",
       "63   3.607172  -2.468565  -5.190783   0.989746 -2.069601 -10.654523   \n",
       "64  -2.875730  -5.993804 -12.542236  -8.285859 -0.797593  -6.813976   \n",
       "65   9.408805   0.141316  -0.821431  -1.418567  3.832415  -2.813458   \n",
       "66  12.242992  -0.532321  -1.793944   2.008286 -4.199604  -3.661440   \n",
       "67  12.863327 -10.227871   8.540395  -4.947164  0.367120  -0.755701   \n",
       "\n",
       "      mfcc_12    mfcc_13  class_encoded  \n",
       "0    5.608082  -5.410851              0  \n",
       "1    2.438568  -6.354579              0  \n",
       "2   14.478795 -10.840614              0  \n",
       "3    7.985336   0.112556              0  \n",
       "4    4.767339   0.963716              0  \n",
       "..        ...        ...            ...  \n",
       "63   0.043760   2.364469              3  \n",
       "64 -10.226942  -7.345919              3  \n",
       "65  -1.262796  -4.410855              3  \n",
       "66  -2.616800  -4.645094              3  \n",
       "67   1.520880  -5.485261              3  \n",
       "\n",
       "[68 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_csv('final.csv')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4894fdd-30d4-40c2-bb3f-954da09c24b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d403eb05-5c43-4d67-b05e-d807d7afa048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set distribution:\n",
      "class_encoded\n",
      "1    0.259259\n",
      "3    0.259259\n",
      "2    0.240741\n",
      "0    0.240741\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set distribution:\n",
      "class_encoded\n",
      "0    0.285714\n",
      "2    0.285714\n",
      "3    0.214286\n",
      "1    0.214286\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df1.iloc[:, :-1] \n",
    "y = df1.iloc[:, -1]   \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "print(\"Training set distribution:\")\n",
    "print(train_df[y.name].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nTest set distribution:\")\n",
    "print(test_df[y.name].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96af0bd8-f594-466e-86a3-31ea9a744083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79723b27-8635-45a0-ae26-b21412b08ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 00m 15s]\n",
      "val_loss: 1.385648488998413\n",
      "\n",
      "Best val_loss So Far: 1.3839061260223389\n",
      "Total elapsed time: 00h 07m 17s\n",
      "Results summary\n",
      "Results in .\\gru_tuning\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "gru_units_0: 64\n",
      "gru_units_1: 16\n",
      "dropout_rate_1: 0.4\n",
      "gru_units_2: 48\n",
      "dropout_rate_2: 0.2\n",
      "optimizer: sgd\n",
      "gru_units_3: 32\n",
      "dropout_rate_3: 0.30000000000000004\n",
      "gru_units_4: 32\n",
      "dropout_rate_4: 0.4\n",
      "gru_units_5: 48\n",
      "dropout_rate_5: 0.4\n",
      "Score: 1.3839061260223389\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "gru_units_0: 48\n",
      "gru_units_1: 32\n",
      "dropout_rate_1: 0.4\n",
      "gru_units_2: 48\n",
      "dropout_rate_2: 0.30000000000000004\n",
      "optimizer: sgd\n",
      "gru_units_3: 48\n",
      "dropout_rate_3: 0.2\n",
      "gru_units_4: 48\n",
      "dropout_rate_4: 0.30000000000000004\n",
      "gru_units_5: 32\n",
      "dropout_rate_5: 0.2\n",
      "Score: 1.3842127323150635\n",
      "\n",
      "Trial 18 summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "gru_units_0: 16\n",
      "gru_units_1: 32\n",
      "dropout_rate_1: 0.30000000000000004\n",
      "gru_units_2: 32\n",
      "dropout_rate_2: 0.4\n",
      "optimizer: sgd\n",
      "gru_units_3: 64\n",
      "dropout_rate_3: 0.2\n",
      "gru_units_4: 64\n",
      "dropout_rate_4: 0.30000000000000004\n",
      "gru_units_5: 16\n",
      "dropout_rate_5: 0.4\n",
      "Score: 1.3844287395477295\n",
      "\n",
      "Trial 17 summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "gru_units_0: 32\n",
      "gru_units_1: 32\n",
      "dropout_rate_1: 0.30000000000000004\n",
      "gru_units_2: 64\n",
      "dropout_rate_2: 0.30000000000000004\n",
      "optimizer: sgd\n",
      "gru_units_3: 32\n",
      "dropout_rate_3: 0.2\n",
      "gru_units_4: 64\n",
      "dropout_rate_4: 0.30000000000000004\n",
      "gru_units_5: 64\n",
      "dropout_rate_5: 0.2\n",
      "Score: 1.3852218389511108\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "num_layers: 5\n",
      "gru_units_0: 48\n",
      "gru_units_1: 32\n",
      "dropout_rate_1: 0.4\n",
      "gru_units_2: 16\n",
      "dropout_rate_2: 0.30000000000000004\n",
      "optimizer: adam\n",
      "gru_units_3: 16\n",
      "dropout_rate_3: 0.4\n",
      "gru_units_4: 48\n",
      "dropout_rate_4: 0.4\n",
      "gru_units_5: 16\n",
      "dropout_rate_5: 0.4\n",
      "Score: 1.3856362104415894\n",
      "\n",
      "Trial 19 summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "gru_units_0: 32\n",
      "gru_units_1: 48\n",
      "dropout_rate_1: 0.30000000000000004\n",
      "gru_units_2: 16\n",
      "dropout_rate_2: 0.4\n",
      "optimizer: rmsprop\n",
      "gru_units_3: 16\n",
      "dropout_rate_3: 0.2\n",
      "gru_units_4: 48\n",
      "dropout_rate_4: 0.2\n",
      "gru_units_5: 32\n",
      "dropout_rate_5: 0.2\n",
      "Score: 1.385648488998413\n",
      "\n",
      "Trial 13 summary\n",
      "Hyperparameters:\n",
      "num_layers: 6\n",
      "gru_units_0: 64\n",
      "gru_units_1: 32\n",
      "dropout_rate_1: 0.2\n",
      "gru_units_2: 16\n",
      "dropout_rate_2: 0.30000000000000004\n",
      "optimizer: sgd\n",
      "gru_units_3: 48\n",
      "dropout_rate_3: 0.4\n",
      "gru_units_4: 64\n",
      "dropout_rate_4: 0.30000000000000004\n",
      "gru_units_5: 32\n",
      "dropout_rate_5: 0.2\n",
      "Score: 1.3859590291976929\n",
      "\n",
      "Trial 15 summary\n",
      "Hyperparameters:\n",
      "num_layers: 6\n",
      "gru_units_0: 48\n",
      "gru_units_1: 48\n",
      "dropout_rate_1: 0.30000000000000004\n",
      "gru_units_2: 48\n",
      "dropout_rate_2: 0.4\n",
      "optimizer: adam\n",
      "gru_units_3: 64\n",
      "dropout_rate_3: 0.30000000000000004\n",
      "gru_units_4: 48\n",
      "dropout_rate_4: 0.30000000000000004\n",
      "gru_units_5: 64\n",
      "dropout_rate_5: 0.4\n",
      "Score: 1.3861467838287354\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "num_layers: 5\n",
      "gru_units_0: 16\n",
      "gru_units_1: 16\n",
      "dropout_rate_1: 0.4\n",
      "gru_units_2: 32\n",
      "dropout_rate_2: 0.30000000000000004\n",
      "optimizer: adam\n",
      "gru_units_3: 64\n",
      "dropout_rate_3: 0.4\n",
      "gru_units_4: 48\n",
      "dropout_rate_4: 0.4\n",
      "gru_units_5: 32\n",
      "dropout_rate_5: 0.30000000000000004\n",
      "Score: 1.3863375186920166\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "num_layers: 6\n",
      "gru_units_0: 16\n",
      "gru_units_1: 16\n",
      "dropout_rate_1: 0.4\n",
      "gru_units_2: 16\n",
      "dropout_rate_2: 0.30000000000000004\n",
      "optimizer: adam\n",
      "gru_units_3: 64\n",
      "dropout_rate_3: 0.4\n",
      "gru_units_4: 32\n",
      "dropout_rate_4: 0.4\n",
      "gru_units_5: 16\n",
      "dropout_rate_5: 0.30000000000000004\n",
      "Score: 1.3866719007492065\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  # Ensure you import pandas if you're using DataFrames\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt  # Use keras_tuner instead of kerastuner\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Assuming X_train, X_test, y_train, and y_test are already defined\n",
    "\n",
    "# Check if they are DataFrames and convert to NumPy arrays if necessary\n",
    "if isinstance(X_train, pd.DataFrame):\n",
    "    X_train = X_train.to_numpy()  # Convert to NumPy array\n",
    "if isinstance(X_test, pd.DataFrame):\n",
    "    X_test = X_test.to_numpy()    # Convert to NumPy array\n",
    "\n",
    "# One-hot encoding the labels\n",
    "num_classes = 4\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Reshape the input data for GRU (samples, timesteps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))  # Shape: (54, 1, 14)\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))      # Shape: (14, 1, 14)\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    num_layers = hp.Int('num_layers', 3, 6)  # Limit to 3 to 6 layers to reduce complexity\n",
    "\n",
    "    # Adding GRU layers\n",
    "    model.add(GRU(hp.Int('gru_units_0', 16, 64, step=16), \n",
    "                   return_sequences=num_layers > 1, \n",
    "                   input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    for i in range(1, num_layers):\n",
    "        model.add(GRU(hp.Int(f'gru_units_{i}', 16, 64, step=16), \n",
    "                       return_sequences=i < num_layers - 1))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # Hyperparameter for dropout rate\n",
    "        dropout_rate = hp.Float(f'dropout_rate_{i}', 0.2, 0.5, step=0.1)\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer for categorical classification\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    optimizer_options = ['adam', 'sgd', 'rmsprop']\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=hp.Choice('optimizer', optimizer_options), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initializing the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    project_name='gru_tuning'\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Start the tuning process with a fixed batch size\n",
    "batch_size = 8\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=50,\n",
    "             validation_data=(X_test, y_test),\n",
    "             batch_size=batch_size,  # Fixed batch size\n",
    "             callbacks=[early_stopping])\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "702d27f2-cab2-4eb6-b3f8-2db5cd81fdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 1, 64)             15168     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 1, 64)            256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 1, 16)             3936      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 1, 16)            64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 16)             0         \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 48)                9504      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 48)               192       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 48)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 196       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,316\n",
      "Trainable params: 29,060\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ed65e23-f1ca-47e9-972a-f2cb74d13499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.3839\n",
      "Test Accuracy: 0.2857\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acc8e72d-a6e7-4212-abc9-22494d9bd10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save('gru.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
