{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b44d11f-239e-4cb5-8f0c-1fd13d22f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588ffd57-ded7-4e22-98ad-dc6acde1b30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>mfcc_1</th>\n",
       "      <th>mfcc_2</th>\n",
       "      <th>mfcc_3</th>\n",
       "      <th>mfcc_4</th>\n",
       "      <th>mfcc_5</th>\n",
       "      <th>mfcc_6</th>\n",
       "      <th>mfcc_7</th>\n",
       "      <th>mfcc_8</th>\n",
       "      <th>mfcc_9</th>\n",
       "      <th>mfcc_10</th>\n",
       "      <th>mfcc_11</th>\n",
       "      <th>mfcc_12</th>\n",
       "      <th>mfcc_13</th>\n",
       "      <th>class_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-596.27124</td>\n",
       "      <td>105.507520</td>\n",
       "      <td>-16.348253</td>\n",
       "      <td>34.213352</td>\n",
       "      <td>-7.040627</td>\n",
       "      <td>9.673249</td>\n",
       "      <td>15.672894</td>\n",
       "      <td>-18.594084</td>\n",
       "      <td>12.680883</td>\n",
       "      <td>-3.053727</td>\n",
       "      <td>-9.320203</td>\n",
       "      <td>5.608082</td>\n",
       "      <td>-5.410851</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-359.78244</td>\n",
       "      <td>72.187680</td>\n",
       "      <td>27.768223</td>\n",
       "      <td>38.328430</td>\n",
       "      <td>5.198799</td>\n",
       "      <td>8.380541</td>\n",
       "      <td>10.607283</td>\n",
       "      <td>-13.184477</td>\n",
       "      <td>11.505762</td>\n",
       "      <td>-4.186356</td>\n",
       "      <td>-6.031159</td>\n",
       "      <td>2.438568</td>\n",
       "      <td>-6.354579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-424.57343</td>\n",
       "      <td>57.615242</td>\n",
       "      <td>12.128860</td>\n",
       "      <td>49.243443</td>\n",
       "      <td>-2.526363</td>\n",
       "      <td>11.702657</td>\n",
       "      <td>11.698957</td>\n",
       "      <td>-2.770733</td>\n",
       "      <td>-1.388484</td>\n",
       "      <td>-1.123289</td>\n",
       "      <td>-11.638446</td>\n",
       "      <td>14.478795</td>\n",
       "      <td>-10.840614</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-524.17444</td>\n",
       "      <td>103.392075</td>\n",
       "      <td>-7.327183</td>\n",
       "      <td>56.391010</td>\n",
       "      <td>10.343275</td>\n",
       "      <td>4.731972</td>\n",
       "      <td>23.960910</td>\n",
       "      <td>-0.654973</td>\n",
       "      <td>6.595408</td>\n",
       "      <td>4.164212</td>\n",
       "      <td>-6.080491</td>\n",
       "      <td>7.985336</td>\n",
       "      <td>0.112556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-519.28940</td>\n",
       "      <td>101.250860</td>\n",
       "      <td>20.647596</td>\n",
       "      <td>29.956661</td>\n",
       "      <td>8.789474</td>\n",
       "      <td>8.442513</td>\n",
       "      <td>3.100049</td>\n",
       "      <td>5.523725</td>\n",
       "      <td>0.429982</td>\n",
       "      <td>5.303805</td>\n",
       "      <td>-0.014299</td>\n",
       "      <td>4.767339</td>\n",
       "      <td>0.963716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>-473.95490</td>\n",
       "      <td>94.092310</td>\n",
       "      <td>20.465342</td>\n",
       "      <td>14.455287</td>\n",
       "      <td>0.151350</td>\n",
       "      <td>3.607172</td>\n",
       "      <td>-2.468565</td>\n",
       "      <td>-5.190783</td>\n",
       "      <td>0.989746</td>\n",
       "      <td>-2.069601</td>\n",
       "      <td>-10.654523</td>\n",
       "      <td>0.043760</td>\n",
       "      <td>2.364469</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>-569.43933</td>\n",
       "      <td>38.809494</td>\n",
       "      <td>15.469359</td>\n",
       "      <td>35.134655</td>\n",
       "      <td>-19.263390</td>\n",
       "      <td>-2.875730</td>\n",
       "      <td>-5.993804</td>\n",
       "      <td>-12.542236</td>\n",
       "      <td>-8.285859</td>\n",
       "      <td>-0.797593</td>\n",
       "      <td>-6.813976</td>\n",
       "      <td>-10.226942</td>\n",
       "      <td>-7.345919</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>-534.11633</td>\n",
       "      <td>45.754597</td>\n",
       "      <td>6.438214</td>\n",
       "      <td>24.977623</td>\n",
       "      <td>1.711067</td>\n",
       "      <td>9.408805</td>\n",
       "      <td>0.141316</td>\n",
       "      <td>-0.821431</td>\n",
       "      <td>-1.418567</td>\n",
       "      <td>3.832415</td>\n",
       "      <td>-2.813458</td>\n",
       "      <td>-1.262796</td>\n",
       "      <td>-4.410855</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>-552.32140</td>\n",
       "      <td>65.580140</td>\n",
       "      <td>21.206110</td>\n",
       "      <td>12.841641</td>\n",
       "      <td>-0.550191</td>\n",
       "      <td>12.242992</td>\n",
       "      <td>-0.532321</td>\n",
       "      <td>-1.793944</td>\n",
       "      <td>2.008286</td>\n",
       "      <td>-4.199604</td>\n",
       "      <td>-3.661440</td>\n",
       "      <td>-2.616800</td>\n",
       "      <td>-4.645094</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>-544.26825</td>\n",
       "      <td>55.101284</td>\n",
       "      <td>16.550330</td>\n",
       "      <td>30.429848</td>\n",
       "      <td>-6.519744</td>\n",
       "      <td>12.863327</td>\n",
       "      <td>-10.227871</td>\n",
       "      <td>8.540395</td>\n",
       "      <td>-4.947164</td>\n",
       "      <td>0.367120</td>\n",
       "      <td>-0.755701</td>\n",
       "      <td>1.520880</td>\n",
       "      <td>-5.485261</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0     mfcc_1      mfcc_2     mfcc_3     mfcc_4     mfcc_5  \\\n",
       "0            0 -596.27124  105.507520 -16.348253  34.213352  -7.040627   \n",
       "1            1 -359.78244   72.187680  27.768223  38.328430   5.198799   \n",
       "2            2 -424.57343   57.615242  12.128860  49.243443  -2.526363   \n",
       "3            3 -524.17444  103.392075  -7.327183  56.391010  10.343275   \n",
       "4            4 -519.28940  101.250860  20.647596  29.956661   8.789474   \n",
       "..         ...        ...         ...        ...        ...        ...   \n",
       "63          63 -473.95490   94.092310  20.465342  14.455287   0.151350   \n",
       "64          64 -569.43933   38.809494  15.469359  35.134655 -19.263390   \n",
       "65          65 -534.11633   45.754597   6.438214  24.977623   1.711067   \n",
       "66          66 -552.32140   65.580140  21.206110  12.841641  -0.550191   \n",
       "67          67 -544.26825   55.101284  16.550330  30.429848  -6.519744   \n",
       "\n",
       "       mfcc_6     mfcc_7     mfcc_8     mfcc_9   mfcc_10    mfcc_11  \\\n",
       "0    9.673249  15.672894 -18.594084  12.680883 -3.053727  -9.320203   \n",
       "1    8.380541  10.607283 -13.184477  11.505762 -4.186356  -6.031159   \n",
       "2   11.702657  11.698957  -2.770733  -1.388484 -1.123289 -11.638446   \n",
       "3    4.731972  23.960910  -0.654973   6.595408  4.164212  -6.080491   \n",
       "4    8.442513   3.100049   5.523725   0.429982  5.303805  -0.014299   \n",
       "..        ...        ...        ...        ...       ...        ...   \n",
       "63   3.607172  -2.468565  -5.190783   0.989746 -2.069601 -10.654523   \n",
       "64  -2.875730  -5.993804 -12.542236  -8.285859 -0.797593  -6.813976   \n",
       "65   9.408805   0.141316  -0.821431  -1.418567  3.832415  -2.813458   \n",
       "66  12.242992  -0.532321  -1.793944   2.008286 -4.199604  -3.661440   \n",
       "67  12.863327 -10.227871   8.540395  -4.947164  0.367120  -0.755701   \n",
       "\n",
       "      mfcc_12    mfcc_13  class_encoded  \n",
       "0    5.608082  -5.410851              0  \n",
       "1    2.438568  -6.354579              0  \n",
       "2   14.478795 -10.840614              0  \n",
       "3    7.985336   0.112556              0  \n",
       "4    4.767339   0.963716              0  \n",
       "..        ...        ...            ...  \n",
       "63   0.043760   2.364469              3  \n",
       "64 -10.226942  -7.345919              3  \n",
       "65  -1.262796  -4.410855              3  \n",
       "66  -2.616800  -4.645094              3  \n",
       "67   1.520880  -5.485261              3  \n",
       "\n",
       "[68 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_csv('final.csv')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea6681c-a658-4fb4-874c-afa6d2d11a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5cc118e-1fbc-40b1-8708-8338a64ba4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set distribution:\n",
      "class_encoded\n",
      "1    0.259259\n",
      "3    0.259259\n",
      "2    0.240741\n",
      "0    0.240741\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set distribution:\n",
      "class_encoded\n",
      "0    0.285714\n",
      "2    0.285714\n",
      "3    0.214286\n",
      "1    0.214286\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df1.iloc[:, :-1] \n",
    "y = df1.iloc[:, -1]   \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "print(\"Training set distribution:\")\n",
    "print(train_df[y.name].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nTest set distribution:\")\n",
    "print(test_df[y.name].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73103bc6-06f5-4b36-9a17-6cbd47756717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b13c9f6-42ce-4754-9e2b-0e35a0c6db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a8dca7f-2d7c-4db7-91b0-2029a1c9dee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 00m 12s]\n",
      "val_loss: 3.3864762783050537\n",
      "\n",
      "Best val_loss So Far: 2.8620076179504395\n",
      "Total elapsed time: 00h 03m 50s\n",
      "Results summary\n",
      "Results in .\\ann_tuning\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 16 summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "input_units: 48\n",
      "dense_units_0: 112\n",
      "activation_0: sigmoid\n",
      "dropout_rate_0: 0.1\n",
      "dense_units_1: 48\n",
      "activation_1: tanh\n",
      "dropout_rate_1: 0.1\n",
      "dense_units_2: 80\n",
      "activation_2: sigmoid\n",
      "dropout_rate_2: 0.4\n",
      "optimizer: adam\n",
      "dense_units_3: 96\n",
      "activation_3: sigmoid\n",
      "dropout_rate_3: 0.1\n",
      "dense_units_4: 32\n",
      "activation_4: relu\n",
      "dropout_rate_4: 0.2\n",
      "dense_units_5: 128\n",
      "activation_5: relu\n",
      "dropout_rate_5: 0.2\n",
      "dense_units_6: 112\n",
      "activation_6: sigmoid\n",
      "dropout_rate_6: 0.30000000000000004\n",
      "dense_units_7: 32\n",
      "activation_7: tanh\n",
      "dropout_rate_7: 0.30000000000000004\n",
      "dense_units_8: 80\n",
      "activation_8: tanh\n",
      "dropout_rate_8: 0.4\n",
      "Score: 2.8620076179504395\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "input_units: 96\n",
      "dense_units_0: 128\n",
      "activation_0: sigmoid\n",
      "dropout_rate_0: 0.30000000000000004\n",
      "dense_units_1: 64\n",
      "activation_1: relu\n",
      "dropout_rate_1: 0.4\n",
      "dense_units_2: 96\n",
      "activation_2: tanh\n",
      "dropout_rate_2: 0.1\n",
      "optimizer: rmsprop\n",
      "Score: 3.3208110332489014\n",
      "\n",
      "Trial 17 summary\n",
      "Hyperparameters:\n",
      "num_layers: 5\n",
      "input_units: 80\n",
      "dense_units_0: 112\n",
      "activation_0: sigmoid\n",
      "dropout_rate_0: 0.4\n",
      "dense_units_1: 48\n",
      "activation_1: sigmoid\n",
      "dropout_rate_1: 0.2\n",
      "dense_units_2: 48\n",
      "activation_2: sigmoid\n",
      "dropout_rate_2: 0.30000000000000004\n",
      "optimizer: rmsprop\n",
      "dense_units_3: 96\n",
      "activation_3: sigmoid\n",
      "dropout_rate_3: 0.30000000000000004\n",
      "dense_units_4: 48\n",
      "activation_4: relu\n",
      "dropout_rate_4: 0.2\n",
      "dense_units_5: 112\n",
      "activation_5: relu\n",
      "dropout_rate_5: 0.0\n",
      "dense_units_6: 48\n",
      "activation_6: sigmoid\n",
      "dropout_rate_6: 0.1\n",
      "dense_units_7: 48\n",
      "activation_7: sigmoid\n",
      "dropout_rate_7: 0.0\n",
      "dense_units_8: 80\n",
      "activation_8: tanh\n",
      "dropout_rate_8: 0.2\n",
      "Score: 3.35392689704895\n",
      "\n",
      "Trial 19 summary\n",
      "Hyperparameters:\n",
      "num_layers: 5\n",
      "input_units: 96\n",
      "dense_units_0: 128\n",
      "activation_0: tanh\n",
      "dropout_rate_0: 0.30000000000000004\n",
      "dense_units_1: 32\n",
      "activation_1: relu\n",
      "dropout_rate_1: 0.4\n",
      "dense_units_2: 48\n",
      "activation_2: relu\n",
      "dropout_rate_2: 0.30000000000000004\n",
      "optimizer: adam\n",
      "dense_units_3: 80\n",
      "activation_3: sigmoid\n",
      "dropout_rate_3: 0.0\n",
      "dense_units_4: 96\n",
      "activation_4: sigmoid\n",
      "dropout_rate_4: 0.2\n",
      "dense_units_5: 48\n",
      "activation_5: sigmoid\n",
      "dropout_rate_5: 0.0\n",
      "dense_units_6: 64\n",
      "activation_6: tanh\n",
      "dropout_rate_6: 0.0\n",
      "dense_units_7: 32\n",
      "activation_7: sigmoid\n",
      "dropout_rate_7: 0.0\n",
      "dense_units_8: 128\n",
      "activation_8: relu\n",
      "dropout_rate_8: 0.1\n",
      "Score: 3.3864762783050537\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "input_units: 80\n",
      "dense_units_0: 48\n",
      "activation_0: tanh\n",
      "dropout_rate_0: 0.30000000000000004\n",
      "dense_units_1: 64\n",
      "activation_1: tanh\n",
      "dropout_rate_1: 0.4\n",
      "dense_units_2: 128\n",
      "activation_2: relu\n",
      "dropout_rate_2: 0.1\n",
      "optimizer: adam\n",
      "dense_units_3: 32\n",
      "activation_3: relu\n",
      "dropout_rate_3: 0.2\n",
      "dense_units_4: 128\n",
      "activation_4: sigmoid\n",
      "dropout_rate_4: 0.4\n",
      "dense_units_5: 128\n",
      "activation_5: relu\n",
      "dropout_rate_5: 0.0\n",
      "dense_units_6: 32\n",
      "activation_6: tanh\n",
      "dropout_rate_6: 0.0\n",
      "dense_units_7: 112\n",
      "activation_7: sigmoid\n",
      "dropout_rate_7: 0.0\n",
      "dense_units_8: 48\n",
      "activation_8: tanh\n",
      "dropout_rate_8: 0.30000000000000004\n",
      "Score: 3.4148330688476562\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "num_layers: 6\n",
      "input_units: 128\n",
      "dense_units_0: 48\n",
      "activation_0: relu\n",
      "dropout_rate_0: 0.30000000000000004\n",
      "dense_units_1: 112\n",
      "activation_1: relu\n",
      "dropout_rate_1: 0.2\n",
      "dense_units_2: 96\n",
      "activation_2: sigmoid\n",
      "dropout_rate_2: 0.0\n",
      "optimizer: rmsprop\n",
      "dense_units_3: 32\n",
      "activation_3: relu\n",
      "dropout_rate_3: 0.0\n",
      "dense_units_4: 32\n",
      "activation_4: relu\n",
      "dropout_rate_4: 0.0\n",
      "Score: 4.096482276916504\n",
      "\n",
      "Trial 15 summary\n",
      "Hyperparameters:\n",
      "num_layers: 5\n",
      "input_units: 32\n",
      "dense_units_0: 32\n",
      "activation_0: tanh\n",
      "dropout_rate_0: 0.1\n",
      "dense_units_1: 80\n",
      "activation_1: sigmoid\n",
      "dropout_rate_1: 0.4\n",
      "dense_units_2: 128\n",
      "activation_2: relu\n",
      "dropout_rate_2: 0.1\n",
      "optimizer: adam\n",
      "dense_units_3: 128\n",
      "activation_3: sigmoid\n",
      "dropout_rate_3: 0.4\n",
      "dense_units_4: 80\n",
      "activation_4: relu\n",
      "dropout_rate_4: 0.0\n",
      "dense_units_5: 64\n",
      "activation_5: sigmoid\n",
      "dropout_rate_5: 0.2\n",
      "dense_units_6: 64\n",
      "activation_6: relu\n",
      "dropout_rate_6: 0.4\n",
      "dense_units_7: 96\n",
      "activation_7: tanh\n",
      "dropout_rate_7: 0.4\n",
      "dense_units_8: 96\n",
      "activation_8: tanh\n",
      "dropout_rate_8: 0.1\n",
      "Score: 4.114675045013428\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "num_layers: 6\n",
      "input_units: 128\n",
      "dense_units_0: 64\n",
      "activation_0: sigmoid\n",
      "dropout_rate_0: 0.1\n",
      "dense_units_1: 32\n",
      "activation_1: sigmoid\n",
      "dropout_rate_1: 0.4\n",
      "dense_units_2: 48\n",
      "activation_2: tanh\n",
      "dropout_rate_2: 0.2\n",
      "optimizer: rmsprop\n",
      "dense_units_3: 112\n",
      "activation_3: relu\n",
      "dropout_rate_3: 0.0\n",
      "dense_units_4: 96\n",
      "activation_4: relu\n",
      "dropout_rate_4: 0.2\n",
      "dense_units_5: 96\n",
      "activation_5: tanh\n",
      "dropout_rate_5: 0.1\n",
      "dense_units_6: 32\n",
      "activation_6: tanh\n",
      "dropout_rate_6: 0.2\n",
      "dense_units_7: 64\n",
      "activation_7: sigmoid\n",
      "dropout_rate_7: 0.0\n",
      "dense_units_8: 80\n",
      "activation_8: relu\n",
      "dropout_rate_8: 0.4\n",
      "Score: 4.372448444366455\n",
      "\n",
      "Trial 11 summary\n",
      "Hyperparameters:\n",
      "num_layers: 5\n",
      "input_units: 112\n",
      "dense_units_0: 96\n",
      "activation_0: sigmoid\n",
      "dropout_rate_0: 0.4\n",
      "dense_units_1: 96\n",
      "activation_1: tanh\n",
      "dropout_rate_1: 0.0\n",
      "dense_units_2: 128\n",
      "activation_2: tanh\n",
      "dropout_rate_2: 0.1\n",
      "optimizer: rmsprop\n",
      "dense_units_3: 96\n",
      "activation_3: sigmoid\n",
      "dropout_rate_3: 0.0\n",
      "dense_units_4: 48\n",
      "activation_4: relu\n",
      "dropout_rate_4: 0.1\n",
      "dense_units_5: 48\n",
      "activation_5: relu\n",
      "dropout_rate_5: 0.1\n",
      "dense_units_6: 48\n",
      "activation_6: tanh\n",
      "dropout_rate_6: 0.30000000000000004\n",
      "dense_units_7: 80\n",
      "activation_7: sigmoid\n",
      "dropout_rate_7: 0.2\n",
      "dense_units_8: 32\n",
      "activation_8: tanh\n",
      "dropout_rate_8: 0.0\n",
      "Score: 4.462569236755371\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "num_layers: 6\n",
      "input_units: 128\n",
      "dense_units_0: 32\n",
      "activation_0: sigmoid\n",
      "dropout_rate_0: 0.2\n",
      "dense_units_1: 64\n",
      "activation_1: relu\n",
      "dropout_rate_1: 0.4\n",
      "dense_units_2: 48\n",
      "activation_2: tanh\n",
      "dropout_rate_2: 0.2\n",
      "optimizer: sgd\n",
      "dense_units_3: 128\n",
      "activation_3: tanh\n",
      "dropout_rate_3: 0.0\n",
      "dense_units_4: 128\n",
      "activation_4: relu\n",
      "dropout_rate_4: 0.4\n",
      "dense_units_5: 80\n",
      "activation_5: sigmoid\n",
      "dropout_rate_5: 0.0\n",
      "dense_units_6: 128\n",
      "activation_6: relu\n",
      "dropout_rate_6: 0.30000000000000004\n",
      "dense_units_7: 64\n",
      "activation_7: sigmoid\n",
      "dropout_rate_7: 0.2\n",
      "dense_units_8: 64\n",
      "activation_8: tanh\n",
      "dropout_rate_8: 0.4\n",
      "Score: 4.845207214355469\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from kerastuner import RandomSearch\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Assuming X_train, X_test, y_train, and y_test are already defined\n",
    "\n",
    "# One-hot encoding the labels\n",
    "num_classes = 4\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    num_layers = hp.Int('num_layers', 4, 10)\n",
    "\n",
    "    # Input layer\n",
    "    input_dim = X_train.shape[1]  # Update to your input feature dimension\n",
    "    model.add(Dense(hp.Int('input_units', 32, 128, step=16), \n",
    "                    activation='relu', \n",
    "                    input_shape=(input_dim,)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    for i in range(num_layers - 1):\n",
    "        num_units = hp.Int(f'dense_units_{i}', 32, 128, step=16)\n",
    "        model.add(Dense(num_units, activation=hp.Choice(f'activation_{i}', ['relu', 'tanh', 'sigmoid']),\n",
    "                        kernel_regularizer='l2'))  # Adding L2 regularization\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        dropout_rate = hp.Float(f'dropout_rate_{i}', 0.0, 0.5, step=0.1)\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    optimizer_options = ['adam', 'sgd', 'rmsprop']\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=hp.Choice('optimizer', optimizer_options), \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initializing the tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    project_name='ann_tuning'\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Adding batch size hyperparameter\n",
    "batch_size = 8\n",
    "\n",
    "# Start the tuning process\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=50,\n",
    "             batch_size=batch_size,  # Use the batch size hyperparameter\n",
    "             validation_data=(X_test, y_test),\n",
    "             callbacks=[early_stopping])\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "tuner.results_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7b35eda-ea27-4901-a4b4-194e1176292c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 48)                672       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 48)               192       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 112)               5488      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 112)              448       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 112)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 48)                5424      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 48)               192       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 48)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 80)                3920      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 80)               320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4)                 324       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,980\n",
      "Trainable params: 16,404\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c40f395-4076-46f1-a8c2-3df0b154216e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.8620\n",
      "Test Accuracy: 0.2857\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1359df78-b480-4688-9f5e-94c887f8d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save('ann.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
